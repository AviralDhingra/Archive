{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import os\n",
    "# os.chdir(r\"/home/deathblade287/Dropbox/ML/Projects/utils\")\n",
    "from utils import define_y_axis, auto_dropper, label_encoder_auto, last\n",
    "# sklearn stands for scikit learn which is pre-defined ML library in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"/home/deathblade287/Dropbox/ML/Projects/insurance_claims\")\n",
    "\n",
    "le=LabelEncoder()\n",
    "scaler=StandardScaler()\n",
    "xgb1=XGBClassifier()\n",
    "\n",
    "\n",
    "# LabelEncoding refers to assigning numeric values to categorical variables (for eg male/female , Yes/No, Sun/Mon/../Sat, etc)\n",
    "# Scaling lets just park for next class\n",
    "\n",
    "# We will use XGBoost algorithm for this\n",
    "# Regression is when we have to predict continuous values for eg 1,1.5,1.6,1.7,1.77,...,2,2.1,2.2,2.3,...,5\n",
    "# Classification is when we have to predict classes - car/bike , male/female , samsung/iphone/microsoft, etc\n",
    "\n",
    "#In this case of Fraud, we have to create model which will predict whether it is fraud or not fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost - Xtreme gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv\n",
    "\n",
    "data = pd.read_csv(r\"insurance_claims_0723_full.csv\")\n",
    "\n",
    "# data is a variable in dataframe\n",
    "data, y = define_y_axis(data, \"fraud_reported\")\n",
    "\n",
    "all_column = auto_dropper(data, y)\n",
    "print(all_column)\n",
    "print(type(all_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform date feature to convert it into day and week\n",
    "data['incident_date']=pd.to_datetime(data.incident_date)\n",
    "data[\"incident_monthdate\"] = data.incident_date.dt.day\n",
    "data[\"incident_weekday\"]=data.incident_date.dt.dayofweek\n",
    "\n",
    "#data_incident_date=data[\"incident_date\"]\n",
    "data=data.drop(\"incident_date\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"incident_monthdate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"incident_weekday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting 100/200 format into 2 columns - one col with values before / and other col with valiues after /\n",
    "\n",
    "csl = data[\"policy_csl\"].str.split(\"/\", n = 1, expand = True)\n",
    "data[\"lower_csl\"]=csl[0]\n",
    "data[\"upper_csl\"]=csl[1]\n",
    "\n",
    "\n",
    "# Use \"Others\" instead of \"?\" and then use labelencoding\n",
    "\n",
    "data[\"collision_type\"].replace(\"?\",\"Others\")\n",
    "data['collision_type']=le.fit_transform(data['collision_type'])\n",
    "\n",
    "data[\"property_damage\"].replace(\"?\",\"Others\")\n",
    "data['property_damage']=le.fit_transform(data['property_damage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"collision_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to use labelencoder wherever the cols have categorical data (not numeric data)\n",
    "\n",
    "# data['insured_education_level']=le.fit_transform(data['insured_education_level'])\n",
    "# data['insured_occupation']=le.fit_transform(data['insured_occupation'])\n",
    "# data['insured_hobbies']=le.fit_transform(data['insured_hobbies'])\n",
    "# data['insured_relationship']=le.fit_transform(data['insured_relationship'])\n",
    "# data['incident_type']=le.fit_transform(data['incident_type'])\n",
    "# data['incident_severity']=le.fit_transform(data['incident_severity'])\n",
    "# data['authorities_contacted']=le.fit_transform(data['authorities_contacted'])\n",
    "# data['incident_state']=le.fit_transform(data['incident_state'])\n",
    "# data['police_report_available']=le.fit_transform(data['police_report_available'])\n",
    "# data['auto_make']=le.fit_transform(data['auto_make'])\n",
    "# data['auto_model']=le.fit_transform(data['auto_model'])\n",
    "# data['auto_year']=le.fit_transform(data['auto_year'])\n",
    "\n",
    "# label_encoder_auto()\n",
    "\n",
    "# os.chdir(path)\n",
    "data = data\n",
    "columns_encoded = []\n",
    "# Lable Encoding (Starting)...\n",
    "le = LabelEncoder()\n",
    "for i in range(len(data.columns)):\n",
    "    column_now = data.columns[i]\n",
    "    all_rows = data[column_now][0:]\n",
    "    # Removing The Index + pandas.core.series.Series => List\n",
    "    all_rows = all_rows.to_list()\n",
    "    for i in range(len(all_rows)):\n",
    "        if type(all_rows[i]) == int:\n",
    "            status_encoding = False\n",
    "        elif type(all_rows[i]) != int:\n",
    "            status_encoding = True\n",
    "            break\n",
    "    if status_encoding == True:\n",
    "        data[column_now] = le.fit_transform(data[column_now])\n",
    "        columns_encoded.append(column_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping those columns which we think will have NO impact/effect on fraud\n",
    "\n",
    "# data=data.drop(\"_c39\",axis=1)\n",
    "\n",
    "\n",
    "# data=data.drop(\"policy_number\",axis=1)\n",
    "# data=data.drop(\"policy_bind_date\",axis=1)\n",
    "# data=data.drop(\"policy_csl\",axis=1)\n",
    "# data=data.drop(\"insured_zip\",axis=1)\n",
    "# data=data.drop(\"insured_sex\",axis=1)\n",
    "# data=data.drop(\"incident_city\",axis=1)\n",
    "# data=data.drop(\"incident_hour_of_the_day\",axis=1)\n",
    "# data=data.drop(\"incident_location\",axis=1)\n",
    "# data=data.drop(\"injury_claim\",axis=1)\n",
    "# data=data.drop(\"property_claim\",axis=1)\n",
    "# data=data.drop(\"age\",axis=1)\n",
    "# data=data.drop(\"policy_state\",axis=1)\n",
    "# data=data.drop(\"policy_deductable\",axis=1)\n",
    "# data=data.drop(\"umbrella_limit\",axis=1)\n",
    "# data=data.drop(\"insured_education_level\",axis=1)\n",
    "# data=data.drop(\"insured_relationship\",axis=1)\n",
    "# data=data.drop(\"capital-gains\",axis=1)\n",
    "# data=data.drop(\"capital-loss\",axis=1)\n",
    "# data=data.drop(\"incident_type\",axis=1)\n",
    "# data=data.drop(\"collision_type\",axis=1)\n",
    "# data=data.drop(\"authorities_contacted\",axis=1)\n",
    "# data=data.drop(\"incident_state\",axis=1)\n",
    "# data=data.drop(\"number_of_vehicles_involved\",axis=1)\n",
    "# data=data.drop(\"property_damage\",axis=1)\n",
    "# data=data.drop(\"witnesses\",axis=1)\n",
    "# data=data.drop(\"auto_model\",axis=1)\n",
    "# data=data.drop(\"auto_year\",axis=1)\n",
    "# data=data.drop(\"lower_csl\",axis=1)\n",
    "# data=data.drop(\"upper_csl\",axis=1)\n",
    "# data=data.drop(\"incident_monthdate\",axis=1)\n",
    "data, y = define_y_axis(data, \"fraud_reported\")\n",
    "auto_dropper(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n",
    "# .describe is used to view/analyze data statistically \n",
    "\n",
    "# 1. Identify outliers with the help of mean, std, max - too high or too low\n",
    "# 2. see if these outliers have a relation with label (fraud or not fraud)\n",
    "# 3. if they have no relation, which mean that these outliers are not frauds, then we can remove the rows\n",
    "# to avoid outliers in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"total_claim_amount\"]>100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_refined = data.columns\n",
    "cols_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to apply scaling before splitting into train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=test_size, random_state=seed,stratify=y)\n",
    "\n",
    "# if we dont use \"y\" within stratify, the split of our labels (fraud yes/no in our case)\n",
    "# can be different from ourt train-test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 4,\n",
    "    'booster':\"gbtree\",\n",
    "    'learning_rate': 0.09,\n",
    "    'n_estimators': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_xgb_1 = XGBClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "# using .fit function means training the algorithm\n",
    "# it means below\n",
    "# 1. Start with random theta for all columns\n",
    "# 2. Calculate cost function\n",
    "# 3. Update theta\n",
    "# 4. repeat above steps until the cost function is closer to 0 or stops reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_data_xgb_1 = model_data_xgb_1.predict(X_test)\n",
    "\n",
    "# capturing the predictions of test data\n",
    "# test data is what algorithm has NOT used for training purpose\n",
    "# it means that model doesnt know the actual answers but it will predict basis the coefficients model has developed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for XGBoost model: %.2f\" % (accuracy_score(y_test, y_pred_data_xgb_1) * 100))\n",
    "\n",
    "# accuracy is calculated by ccmparing actual labels with predicted labels of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_data_xgb_1.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(cols_refined,model_data_xgb_1.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pickle library to store/save the model\n",
    "# .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use the model in the actual environment\n",
    "# 1. Store the model (pickle)\n",
    "# 2. Create an application (for eg website) which takes 12 cols from the customer while he/she logs a claim\n",
    "# 3. apply the model on 12 cols (.predict)\n",
    "# 4 finally the outcome is either of the class - fraud or not fraud\n",
    "# 5. an action can be taken for eg, an email can be sent automatically to the investigation team (insurance company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next class\n",
    "# 1. Explain parameters\n",
    "# 2. Scaling\n",
    "# 3. deleting rows\n",
    "# 4. demonstrate complete cycle of applying model\n",
    "# 5. how to use .describe function\n",
    "\n",
    "\n",
    "# The standard score of a sample x is calculated as:\n",
    "# z = (x - u) / s\n",
    "\n",
    "\n",
    "# where u is the mean of the training samples\n",
    "# and s is the standard deviation of the training samples\n",
    "\n",
    "# scaling converts all numbers with mean as 0 and std as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment for July 30th is to try and create model with all cols - dont delete any col\n",
    "# observe the feature_importance when you input all cols\n",
    "# dont forget to do label encoding and scaling on all cols\n",
    "# take reference from above example\n",
    "\n",
    "# In fact, change any parameter and see how the accuracy increases or decreases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
